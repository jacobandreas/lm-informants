---
title: "FussWithJacobOutput"
author: "Canaan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidylog)
library(jsonlite)
library(cowplot)
setwd("G:/My Drive/MIT/Research/Active Learning with Roger and Jacob Andreas/JacobCode")

```



```{r}
words <- read_csv("WordsAndScores.csv")
scores <- read_csv("HumanEvalLogs.csv") %>% 
  rename(Word = Item)

oracle <- scores %>% 
  filter(Source == "JUDGE") 

learner <- scores %>% 
  filter(Source == "LEARNER")

full <- left_join(words,learner, by = "Word") %>% 
  distinct() %>% 
  rename(Source = Source.x)

oracles <- left_join(words,oracle,by = "Word") %>% 
  distinct() %>% 
  rename(Source = Source.x) %>% 
  group_by(Source,Step,Strategy,N_INIT) %>% 
  summarise(r=cor(Score,Cost)) %>% 
  ungroup() %>% 
  select(Source, r) %>% 
  distinct()

a <- full %>% 
  group_by(Source,Step,Strategy,N_INIT) %>% 
  summarise(r=cor(Score,Cost))

a %>% 
  ggplot(aes(x=Step,y=r, group=Strategy, color = Strategy))+
  geom_smooth()+
  theme_bw()+
  facet_wrap(~N_INIT*Source,ncol = 3)

ggsave("EvalJudgments.jpeg",height=10,width=10)


ah <- a %>% 
  filter(Source == "AH_2003",
  #       Strategy == "train"
  ) %>% 
  ggplot(aes(x=Step,y=r, group=Strategy, color = Strategy))+
  geom_hline(yintercept = as.numeric(oracles[1,2])*-1, linetype = "dashed", color = "red")+
  geom_smooth()+
  ylim(-1,1)+
  theme_bw()+
  theme(legend.position = "none")+
  facet_wrap(~Source)
ah
jlhw <- a %>% 
  filter(Source == "JLHW_Canaan",
  #       Strategy == "train"
  ) %>% 
  ggplot(aes(x=Step,y=r, group=Strategy, color = Strategy))+
  geom_hline(yintercept = as.numeric(oracles[2,2])*-1, linetype = "dashed", color = "red")+
  geom_smooth()+
  ylim(-1,1)+
  theme_bw()+
  theme(legend.position = "none")+
  facet_wrap(~Source)
jlhw
scholes <- a %>% 
  filter(Source == "Scholes",
  #       Strategy == "train"
  ) %>% 
  ggplot(aes(x=Step,y=r, group=Strategy, color = Strategy))+
  geom_hline(yintercept = as.numeric(oracles[3,2])*-1, linetype = "dashed", color = "red")+
  geom_smooth()+
  ylim(-1,1)+
  theme_bw()+
  theme(legend.position = "right")+
  facet_wrap(~Source)
scholes

p <- plot_grid(ah,jlhw,scholes, nrow = 1, rel_widths = c(1,1,1.5))
p

ggsave("EvalJudgments_WithInformant.jpeg",height=7,width=10)

```

# now do model evals

```{r}

eval_metrics <- read_csv("ModelEvalLogs.csv")

eval_metrics_long <- eval_metrics %>% 
  pivot_longer(cols = c(ent:rej), names_to = "name",values_to = "vals") %>% 
  group_by(name) %>% 
  mutate(vals_scaled = scale(vals)) %>% 
  #group_by(Source,Step,Strategy,N_INIT) %>% 
  #summarise(r=cor(Score,Cost)) %>% 
  #ungroup() %>% 
  #select(Source, r) %>% 
  distinct()

eval_metrics_long %>% 
  filter(name == "diff") %>% 
  ggplot(aes(x=Step,y = vals, group = N_Init, color = as.factor(N_Init)))+
  geom_smooth()+
  facet_wrap(~Strategy*name, ncol = 6)+
  theme_bw()

ggsave("EvalMetrics.jpeg",width=14, height = 10)

```

# check whether you get a tipping-point datapoint
```{r}
feat_probs <- read_csv("FeatureProbs.csv", col_names = F) %>% 
  rename(N_INIT = X1,
    FeatProb = X2,
         Feature = X3,
         Step = X4,
         Candidate = X5,
         Judgement = X6,
         Strategy = X7)
```

over time the features where we know nothing should diminish; so x1=.5 should go down.

```{r}
feat_probs %>% 
  #filter()
  select(FeatProb,Step,Candidate,Feature,Strategy,N_INIT) %>% 
  mutate(Direction = case_when(FeatProb == 0.5 ~ "Uninformed",
                               FeatProb > 0.5 ~ "Higher",
                               FeatProb < 0.5 ~ "Lower"
           ))%>% 
  group_by(Step, Strategy,Direction,N_INIT) %>% 
  summarise(n = n()) %>% 
  distinct() %>% 
  ggplot(aes(x=Step, y = n, group = Direction, color = Direction))+
  geom_smooth()+
  facet_wrap(~Strategy*N_INIT, ncol = 4)+
  theme_bw()

# so the features should sum to 64 at each step

feat_probs %>% 
  select(Feature) %>% 
  distinct() %>% 
  count()
```

the features that we know are good (low) are +atr+atr, -atr-atr; the ones we know should be bad (high) are -atr+atr,+atr-atr.


```{r}
feat_probs %>% 
  mutate(FeatureType = if_else(Feature %in% c("+atr :: -atr", "-atr :: +atr"), "ShouldBeHigh_Bad","ShouldBeLow_Good")) %>% 
  group_by(FeatureType,Strategy,Step,N_INIT) %>% 
  summarise(AverageFeatProb = mean(FeatProb)) %>% 
  ungroup() %>% 
  distinct() %>% 
  ggplot(aes(x=Step, y = AverageFeatProb, group = FeatureType,color = FeatureType))+
  geom_smooth()+
  ylim(0,1)+
  facet_wrap(~Strategy*N_INIT,ncol = 4)+
  theme_bw()
```

okay, so, broadly true if we get negative judgments, but it doesn't change under non-informativeness.

so the learner *should*, as soon as we have evidence for the licensed feature combo, push to zero, but all the other cases should be bumped up a bit, right?


it seems like there's the issue of rejections not being bumped up enough (or at all)

ALSO, in the train-only, we should see the same sequence over runs, right? 

```{r}
a <- feat_probs %>% 
  filter(Strategy=="train") %>% 
  select(Candidate,Step) %>% 
  #distinct() %>% 
  group_by(Step) %>% 
  mutate(p = n_distinct(Candidate),
         n = n()) %>% 
  ungroup() %>% 
  distinct()
  
```

so this works over multiple runs of the train, does it work over multiple N_Inits?

yes!


up is bad, down is good

so, do we see a sudden change after the first rejection judgment?

```{r}
feat_probs %>% 
  filter(N_INIT == 0) %>% 
  mutate(FeatureType = if_else(Feature %in% c("+atr :: -atr", "-atr :: +atr"), "ShouldBeHigh_Bad","ShouldBeLow_Good")) %>% 
  group_by(FeatureType,Strategy,Step) %>% 
  summarise(AverageFeatProb = mean(FeatProb),
            J = Judgement) %>% 
  ungroup() %>% 
  distinct() %>% 
  #pivot_wider(names_from = FeatureType, values_from = AverageFeatProb) %>% 
  ggplot(aes(x=Step, y = AverageFeatProb, group = J,color = J))+
  geom_point(aes(shape = FeatureType))+
  ylim(0,1)+
  facet_wrap(~Strategy)+
  theme_bw()


```
now zoom into the first 32 or so

```{r}

feat_probs %>% 
  filter(N_INIT == 0,
         Step < 32) %>% 
  mutate(FeatureType = if_else(Feature %in% c("+atr :: -atr", "-atr :: +atr"), "ShouldBeHigh_Bad","ShouldBeLow_Good")) %>% 
  group_by(FeatureType,Strategy,Step) %>% 
  summarise(AverageFeatProb = mean(FeatProb),
            J = Judgement) %>% 
  ungroup() %>% 
  distinct() %>% 
  #pivot_wider(names_from = FeatureType, values_from = AverageFeatProb) %>% 
  ggplot(aes(x=Step, y = AverageFeatProb, group = J,color = J))+
  geom_point(aes(shape = FeatureType))+
  ylim(0,1)+
  facet_wrap(~Strategy)+
  theme_bw()

```

# analysis of synthetic characteristics of data

```{r}
corpus_stats <- read_csv("analysis_of_characteristics_of_synthetic_data.csv") %>% 
  mutate(NSylls = str_count(WordContainingIt, " ")+1)


```

how rare are telling sequences?

```{r}
s <- corpus_stats %>% 
  select(-WordContainingIt,-TellingSubsequence) %>% 
  group_by(NSylls,ProperJudgementForWord) %>% 
  summarise(n = n()) %>% 
  pivot_wider(values_from = n, names_from = ProperJudgementForWord) %>% 
  mutate(percent_telling_tokens = bad/(okay+bad))


```



so according to this, telling examples that /a/ is opaque are very rare. but that's in the true corpus, not hard to find in the proposals, right? that's the plot above.

I guess the question is how often until we discover that; right? its easy to learn that there is atr harmony, and that a doesn't participate, but less obvious that a is transparent?


# see how the model feels about the space of possible words over time - extensional evaluatino

```{r}
words <- read_csv("WordsAndScores_atr.csv") %>% 
  filter(!is.na(Word)) %>% 
  mutate(TypeOfEvidence = case_when(
    OkUnderTransparentHarmony == TRUE & OkUnderOpaqueHarmony == TRUE ~ "OkUnderBoth",
    OkUnderTransparentHarmony == FALSE & OkUnderOpaqueHarmony == TRUE ~ "OnlyOkUnderOpaqueHarmony",
    OkUnderTransparentHarmony == FALSE & OkUnderOpaqueHarmony == FALSE ~ "BadUnderBoth",
    OkUnderTransparentHarmony == TRUE & OkUnderOpaqueHarmony == FALSE ~ "ERROR"
  )) %>% 
  select(Word,JudgeScore,TypeOfEvidence)

scores <- read_csv("HumanEvalLogs.csv") %>% 
  rename(Word = Item) %>% 
  select(-Source)

full <- left_join(words,scores, by = "Word")

full %>% 
  filter(N_INIT == 0,
         Step <=10) %>% 
  ggplot(aes(x=Step, y = Cost, group = TypeOfEvidence, color = TypeOfEvidence))+
  geom_smooth()+
  facet_wrap(~Strategy, scales = "free_y")+
  theme_bw()
ggsave("EvalJudgments.jpeg",height=10,width=10)


ah <- a %>% 
  filter(Source == "AH_2003",
  #       Strategy == "train"
  ) %>% 
  ggplot(aes(x=Step,y=r, group=Strategy, color = Strategy))+
  geom_hline(yintercept = as.numeric(oracles[1,2])*-1, linetype = "dashed", color = "red")+
  geom_smooth()+
  ylim(-1,1)+
  theme_bw()+
  theme(legend.position = "none")+
  facet_wrap(~Source)
ah
jlhw <- a %>% 
  filter(Source == "JLHW_Canaan",
  #       Strategy == "train"
  ) %>% 
  ggplot(aes(x=Step,y=r, group=Strategy, color = Strategy))+
  geom_hline(yintercept = as.numeric(oracles[2,2])*-1, linetype = "dashed", color = "red")+
  geom_smooth()+
  ylim(-1,1)+
  theme_bw()+
  theme(legend.position = "none")+
  facet_wrap(~Source)
jlhw
scholes <- a %>% 
  filter(Source == "Scholes",
  #       Strategy == "train"
  ) %>% 
  ggplot(aes(x=Step,y=r, group=Strategy, color = Strategy))+
  geom_hline(yintercept = as.numeric(oracles[3,2])*-1, linetype = "dashed", color = "red")+
  geom_smooth()+
  ylim(-1,1)+
  theme_bw()+
  theme(legend.position = "right")+
  facet_wrap(~Source)
scholes

p <- plot_grid(ah,jlhw,scholes, nrow = 1, rel_widths = c(1,1,1.5))
p

ggsave("EvalJudgments_WithInformant.jpeg",height=7,width=10)

```

so it looks like train isn't learning to punish anything, this is the no negative evidence issue? unsure.

