{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55821976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import read_in_blicks, BOUNDARY\n",
    "import scorers\n",
    "import datasets\n",
    "import informants\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8aa0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d16e96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = 'atr_harmony'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ada73e",
   "metadata": {},
   "source": [
    "## Load dataset, scorers, and oracle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a27d2d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lexicon from:\tdata/hw/atr_harmony_lexicon.txt\n",
      "Loading lexicon with min_length=2, max_length=5...\n",
      "Reading phoneme features from: data/hw/atr_harmony_features.txt\n",
      "# features:  512\n",
      "feature type:  atr_harmony\n",
      "Reading phoneme features from: data/hw/atr_harmony_features.txt\n",
      "Loading ngram features from: data/hw\\atr_harmony_feature_weights.txt\n"
     ]
    }
   ],
   "source": [
    "# Change these paths if you want to specify a different set of features\n",
    "lexicon_path = f'data/hw/{feature_type}_lexicon.txt'\n",
    "phoneme_feature_path = f'data/hw/{feature_type}_features.txt'\n",
    "ngram_feature_path = f'data/hw/{feature_type}_feature_weights.txt'\n",
    "\n",
    "print(f'Loading lexicon from:\\t{lexicon_path}')\n",
    "dataset = datasets.load_lexicon(lexicon_path, min_length=2, max_length=5)\n",
    "\n",
    "mf_scorer = scorers.MeanFieldScorer(dataset, \n",
    "                                    feature_type=feature_type, \n",
    "                                    phoneme_feature_file=phoneme_feature_path,\n",
    "                                   )\n",
    "hw_scorer = scorers.HWScorer(dataset, \n",
    "                                    feature_type=feature_type, \n",
    "                                    phoneme_feature_file=phoneme_feature_path,\n",
    "                            )\n",
    "\n",
    "# Load oracle\n",
    "informant = informants.HWInformant(dataset, hw_scorer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e8f7b",
   "metadata": {},
   "source": [
    "# Generate data\n",
    "\n",
    "Here's a function to generate a random string of syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "17e5d126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n",
      "[394, 430, 41, 265, 497, 414, 310, 488, 366, 223, 142, 288, 143, 97, 256, 150, 317, 101]\n",
      "Reading eval items from:\tatr_harmony_TEMP.txt\n",
      "[['ka', 'tE', 'qI'], ['qe', 'pi', 'kE', 'tI'], ['tE', 'qa'], ['pe'], ['qe'], ['qI'], ['kI'], ['qi'], ['ke'], ['ta']]\n",
      "in make train adn test, here are words [['ka', 'tE', 'qI'], ['qe', 'pi', 'kE', 'tI'], ['tE', 'qa'], ['pe'], ['qe'], ['qI'], ['kI'], ['qi'], ['ke'], ['ta']]\n",
      "(0, 3, 5, 18, 0)\n",
      "[ 70  72  75  77  88  91  93  94 104 107 109 110 136 139 141 152 155 157\n",
      " 168 171 173 198 222 238 326 350 366 393 395 397 401 403 405]\n",
      "{366}\n",
      "(0, 7, 6, 1, 2, 0)\n",
      "[  8  11  13  24  27  29  40  43  45  65  67  69  70  89  91  93  94  97\n",
      "  99 101 110 193 195 197 198 200 203 205 216 217 219 221 222 225 227 229\n",
      " 232 235 237 238 257 259 261 264 267 269 280 281 283 285 289 291 293 296\n",
      " 299 301 326 350 366 392 395 396 408 411 412 416 419 420]\n",
      "{97, 101, 366}\n",
      "(0, 5, 13, 0)\n",
      "[ 78  86 206 214 334 342 393 394 409 410 425 426]\n",
      "{394}\n",
      "(0, 16, 0)\n",
      "[398 414 422]\n",
      "{414}\n",
      "(0, 7, 0)\n",
      "[398 414 422]\n",
      "{414}\n",
      "(0, 18, 0)\n",
      "[390 414 430]\n",
      "{430, 414}\n",
      "(0, 10, 0)\n",
      "[390 414 430]\n",
      "{430, 414}\n",
      "(0, 19, 0)\n",
      "[390 414 422]\n",
      "{414}\n",
      "(0, 14, 0)\n",
      "[398 414 422]\n",
      "{414}\n",
      "(0, 12, 0)\n",
      "[398 406]\n",
      "set()\n",
      "[('ta', True)]\n",
      "[]\n",
      "('ta', True)\n",
      "<class 'tuple'>\n",
      "('ke', False)\n",
      "<class 'tuple'>\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n",
      "[137, 64, 261, 120, 507, 460, 483, 388, 214, 96, 499, 29, 399, 443, 2, 456, 272, 234]\n",
      "Reading eval items from:\tatr_harmony_TEMP.txt\n",
      "[['qE'], ['pi'], ['tE'], ['pa'], ['pa', 'pa'], ['qe', 'pa', 'pI'], ['pe'], ['tI', 'pa'], ['qi'], ['te', 'kI', 'qI', 'qe']]\n",
      "in make train adn test, here are words [['qE'], ['pi'], ['tE'], ['pa'], ['pa', 'pa'], ['qe', 'pa', 'pI'], ['pe'], ['tI', 'pa'], ['qi'], ['te', 'kI', 'qI', 'qe']]\n",
      "(0, 20, 0)\n",
      "[398 414 430]\n",
      "set()\n",
      "(0, 6, 0)\n",
      "[390 414 422]\n",
      "set()\n",
      "(0, 5, 0)\n",
      "[398 414 430]\n",
      "set()\n",
      "(0, 8, 0)\n",
      "[398 406]\n",
      "set()\n",
      "(0, 8, 8, 0)\n",
      "[ 78  86 142 150 393 394 401 402]\n",
      "set()\n",
      "(0, 7, 8, 11, 0)\n",
      "[ 70  72  75  77  80  83  85  94 110 134 158 174 200 203 205 208 211 213\n",
      " 264 267 269 272 275 277 393 394 409 410 417 418]\n",
      "{272}\n",
      "(0, 16, 0)\n",
      "[398 414 422]\n",
      "set()\n",
      "(0, 2, 8, 0)\n",
      "[ 14  22 206 214 334 342 385 386 409 410 425 426]\n",
      "{214}\n",
      "(0, 19, 0)\n",
      "[390 414 422]\n",
      "set()\n",
      "(0, 17, 10, 18, 7, 0)\n",
      "[  1   3   4  14  25  27  28  30  38  41  43  44  64  67  69  88  91  93\n",
      " 104 107 109 192 193 195 196 197 206 216 217 219 220 221 222 230 232 233\n",
      " 235 236 237 256 259 261 280 283 285 296 299 301 321 323 324 334 345 347\n",
      " 348 350 358 361 363 364 392 395 397 408 411 413 416 419 421]\n",
      "{64, 261}\n",
      "[('qE', True), ('pi', True), ('tE', True), ('pa', True), ('pa pa', True), ('pe', True), ('qi', True)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36744\\679054471.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[0mnum_languages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_bad_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumerical_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msyllables\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_languages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36744\\679054471.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(num_words, num_bad_features, numerical_features, syllables, num_languages)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbad_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_a_language\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumerical_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msyllables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_bad_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mwrite_out_a_language\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbad_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36744\\679054471.py\u001b[0m in \u001b[0;36mmake_a_language\u001b[1;34m(numerical_features, syllables, num_words, num_bad_features)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbad_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_a_train_and_test_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbad_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msyllables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbad_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36744\\679054471.py\u001b[0m in \u001b[0;36mmake_a_train_and_test_set\u001b[1;34m(num_words, list_of_forbidden_features, syllables)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgoods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgoods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mtest_goods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgoods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgoods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mtest_bads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_goods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_goods\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtest_bads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\random.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, population, k, counts)\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m21\u001b[0m        \u001b[1;31m# size of a small set minus size of an empty list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def make_words(syllables,num_words,list_to_exclude=[]):\n",
    "    word_list = []\n",
    "    while len(word_list) < num_words:\n",
    "        syllable_list = []\n",
    "        length = np.random.poisson(2)\n",
    "        for _ in range(length) if length > 0 else range(1):\n",
    "            next_syllable = random.choice(syllables)\n",
    "            syllable_list.append(next_syllable)\n",
    "        word = \" \".join(syllable_list)\n",
    "        if word not in word_list and word not in list_to_exclude:\n",
    "            word_list.append(word)\n",
    "    temp_out = open(\"atr_harmony_TEMP.txt\",\"w\")\n",
    "    for item in word_list:\n",
    "        temp_out.write(item+\"\\n\")\n",
    "    temp_out.close()\n",
    "    eval_dataset_path = f'{feature_type}_TEMP.txt'\n",
    "    print(f'Reading eval items from:\\t{eval_dataset_path}')\n",
    "    items = read_in_blicks(eval_dataset_path)\n",
    "    print(items)\n",
    "    return items # returns a list of lists of syllables, ex.:[['ti'], ['qe', 'ka', 'qE'], ['ki', 'qI', 'qi', 'qa', 'qi', 'ke', 'ta'], ...]\n",
    "\n",
    "def classify_word(list_of_forbidden_features,word):\n",
    "    phonemes = [BOUNDARY] + word + [BOUNDARY]\n",
    "    # Encode items\n",
    "    encoded_item = dataset.vocab.encode(phonemes)\n",
    "# Get labels with HW oracle\n",
    "    print(encoded_item)\n",
    "    features = mf_scorer._featurize(encoded_item).nonzero()[0]\n",
    "    print(features)\n",
    "    #assert False\n",
    "    print(set.intersection(set(features), set(list_of_forbidden_features)))\n",
    "    #assert False\n",
    "    if len(set.intersection(set(features), set(list_of_forbidden_features))) != 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def make_a_train_and_test_set(num_words,list_of_forbidden_features,syllables):\n",
    "    words = make_words(syllables, num_words)\n",
    "    print(\"in make train adn test, here are words\",words)\n",
    "    goods = []\n",
    "    bads = []\n",
    "    for word in words: # word is a list like ['ti'] or  ['qe', 'ka', 'qE'], etc.\n",
    "        if classify_word(list_of_forbidden_features,word):\n",
    "            goods.append((\" \".join(word),True))\n",
    "        else:\n",
    "            bads.append((\" \".join(word),False))\n",
    "    print(goods)\n",
    "    #assert False\n",
    "    random.shuffle(goods)\n",
    "    train = goods[:len(goods)//2]\n",
    "    test_goods = goods[len(goods)//2:]\n",
    "    test_bads = random.sample(bads,len(test_goods))\n",
    "    return train, test_goods+test_bads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_a_language(numerical_features,syllables,num_words,num_bad_features):\n",
    "    print(numerical_features)\n",
    "    bad_features = random.sample(numerical_features,num_bad_features)\n",
    "    print(bad_features)\n",
    "    \n",
    "    train, test = make_a_train_and_test_set(num_words, bad_features,syllables)\n",
    "    return train, test, bad_features\n",
    "\n",
    "def write_out_a_language(train, test, bad_features, seed):\n",
    "    t = open(\"./ProcGenLgs/atr_lg_with_seed_\"+str(seed)+\".csv\",\"w\")\n",
    "    t.write(\"Word,Set,Status,BadFeatures,Seed\\n\")\n",
    "    for item in train:\n",
    "        t.write(str(item[0])+',Train,True,'+str(bad_features)+\",\"+str(seed)+'\\n')\n",
    "    for item in test:\n",
    "        print(item)\n",
    "        print(type(item))\n",
    "        #assert False\n",
    "        t.write(str(item[0])+',Test,'+str(item[1])+\",\"+str(bad_features)+\",\"+str(seed)+'\\n')\n",
    "    t.close()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def main(num_words, num_bad_features,numerical_features,syllables,num_languages):\n",
    "    for i in range(num_languages):\n",
    "        seed = i\n",
    "        random.seed(seed)\n",
    "        train, test, bad_features = make_a_language(numerical_features,syllables, num_words, num_bad_features)\n",
    "        print(train)\n",
    "        write_out_a_language(train, test, bad_features, seed)\n",
    "        \n",
    "\n",
    "numerical_features = [i for i in mf_scorer.ngram_features.values()]\n",
    "syllables = [\n",
    "    'pa',\n",
    "'ta',\n",
    "'ka',\n",
    "'pi',\n",
    "'ti',\n",
    "'ki',\n",
    "'pe',\n",
    "'te',\n",
    "'ke',\n",
    "'pE',\n",
    "'tE',\n",
    "'kE',\n",
    "'pI',\n",
    "'tI',\n",
    "'kI',\n",
    "'qI',\n",
    "'qi',\n",
    "'qe',\n",
    "'qE',\n",
    "'qa'\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "num_words = 10\n",
    "num_bad_features = 18\n",
    "\n",
    "num_languages = 10\n",
    "\n",
    "main(num_words,num_bad_features,numerical_features,syllables,num_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a543e59",
   "metadata": {},
   "source": [
    "## Getting mean features by labels across full eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b6f36348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading eval items from:\tatr_harmony_test_set.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'atr_harmony_test_set.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36744\\62744111.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0meval_dataset_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{feature_type}_test_set.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Reading eval items from:\\t{eval_dataset_path}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_in_blicks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_dataset_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Google Drive\\My Drive\\MIT\\Research\\lm-informants\\main.py\u001b[0m in \u001b[0;36mread_in_blicks\u001b[1;34m(path_to_wugs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_in_blicks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_wugs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[0mintext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_wugs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m     \u001b[1;31m#print(\"returning blicks\", intext,\"from path\",path_to_wugs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mintext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'atr_harmony_test_set.txt'"
     ]
    }
   ],
   "source": [
    "# Read in items to featurize\n",
    "# Change this path if you want to specify a different eval dataset\n",
    "eval_dataset_path = f'{feature_type}_test_set.txt'\n",
    "print(f'Reading eval items from:\\t{eval_dataset_path}')\n",
    "items = read_in_blicks(eval_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fa746b34",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'g'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36744\\1492729317.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mphonemes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBOUNDARY\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mBOUNDARY\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Encode items\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mencoded_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphon\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mphon\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphonemes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Get labels with HW oracle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minformant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjudge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mencod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded_items\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36744\\1492729317.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mphonemes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBOUNDARY\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mBOUNDARY\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Encode items\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mencoded_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphon\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mphon\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphonemes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Get labels with HW oracle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minformant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjudge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mencod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded_items\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\My Drive\\MIT\\Research\\lm-informants\\datasets.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, tokens, add)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\My Drive\\MIT\\Research\\lm-informants\\datasets.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'g'"
     ]
    }
   ],
   "source": [
    "# Get phonemes\n",
    "phonemes = [[BOUNDARY] + item + [BOUNDARY] for item in items]\n",
    "# Encode items\n",
    "encoded_items = [dataset.vocab.encode(phon) for phon in phonemes]\n",
    "# Get labels with HW oracle\n",
    "labels = [informant.judge(encod) for encod in encoded_items]\n",
    "# Featurize items\n",
    "featurized_items = [mf_scorer._featurize(encod).nonzero()[0] for encod in encoded_items]\n",
    "# Get num features\n",
    "num_features = [len(f) for f in featurized_items]\n",
    "\n",
    "# Get dataframe of results\n",
    "eval_dataset = pd.DataFrame({\n",
    "    'item': items,\n",
    "    'label': labels,\n",
    "    'encoded': encoded_items,\n",
    "    'featurized': featurized_items,\n",
    "    'num_features': num_features,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccac1a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded</th>\n",
       "      <th>featurized</th>\n",
       "      <th>num_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[g, A, I, i]</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 1, 3, 9, 7, 0)</td>\n",
       "      <td>[222, 250, 278, 292, 309, 311, 313, 314, 317, ...</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[H, A, A, H, A, f]</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 5, 3, 3, 5, 3, 8, 0)</td>\n",
       "      <td>[308, 311, 313, 315, 316, 317, 319, 700, 703, ...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[F, f, F, f]</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 6, 8, 6, 8, 0)</td>\n",
       "      <td>[0, 3, 5, 7, 9, 10, 11, 12, 42, 45, 47, 49, 51...</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[A, f, A, f]</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 3, 8, 3, 8, 0)</td>\n",
       "      <td>[112, 115, 117, 119, 121, 123, 700, 703, 705, ...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[A, F, A, i, A]</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 3, 6, 3, 7, 3, 0)</td>\n",
       "      <td>[113, 115, 117, 118, 121, 123, 320, 701, 703, ...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>[A, A, A, g, f]</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 3, 3, 3, 1, 8, 0)</td>\n",
       "      <td>[208, 250, 278, 306, 334, 362, 404, 446, 474, ...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>[G, g, H, h]</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 2, 1, 5, 4, 0)</td>\n",
       "      <td>[211, 213, 214, 217, 219, 220, 221, 222, 225, ...</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>[i, G, i, A]</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 7, 2, 7, 3, 0)</td>\n",
       "      <td>[211, 213, 215, 216, 218, 219, 221, 225, 227, ...</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>[A, F, h, H, F, G]</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 3, 6, 4, 5, 6, 2, 0)</td>\n",
       "      <td>[15, 17, 18, 21, 23, 24, 26, 40, 43, 45, 46, 4...</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>[F, h, G]</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 6, 4, 2, 0)</td>\n",
       "      <td>[15, 16, 19, 21, 23, 24, 43, 44, 47, 49, 51, 5...</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1784 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    item  label                   encoded  \\\n",
       "0           [g, A, I, i]   True        (0, 1, 3, 9, 7, 0)   \n",
       "1     [H, A, A, H, A, f]   True  (0, 5, 3, 3, 5, 3, 8, 0)   \n",
       "2           [F, f, F, f]   True        (0, 6, 8, 6, 8, 0)   \n",
       "3           [A, f, A, f]   True        (0, 3, 8, 3, 8, 0)   \n",
       "4        [A, F, A, i, A]   True     (0, 3, 6, 3, 7, 3, 0)   \n",
       "...                  ...    ...                       ...   \n",
       "1779     [A, A, A, g, f]  False     (0, 3, 3, 3, 1, 8, 0)   \n",
       "1780        [G, g, H, h]  False        (0, 2, 1, 5, 4, 0)   \n",
       "1781        [i, G, i, A]  False        (0, 7, 2, 7, 3, 0)   \n",
       "1782  [A, F, h, H, F, G]  False  (0, 3, 6, 4, 5, 6, 2, 0)   \n",
       "1783           [F, h, G]  False           (0, 6, 4, 2, 0)   \n",
       "\n",
       "                                             featurized  num_features  \n",
       "0     [222, 250, 278, 292, 309, 311, 313, 314, 317, ...           114  \n",
       "1     [308, 311, 313, 315, 316, 317, 319, 700, 703, ...            66  \n",
       "2     [0, 3, 5, 7, 9, 10, 11, 12, 42, 45, 47, 49, 51...           379  \n",
       "3     [112, 115, 117, 119, 121, 123, 700, 703, 705, ...            54  \n",
       "4     [113, 115, 117, 118, 121, 123, 320, 701, 703, ...            57  \n",
       "...                                                 ...           ...  \n",
       "1779  [208, 250, 278, 306, 334, 362, 404, 446, 474, ...            80  \n",
       "1780  [211, 213, 214, 217, 219, 220, 221, 222, 225, ...           429  \n",
       "1781  [211, 213, 215, 216, 218, 219, 221, 225, 227, ...           294  \n",
       "1782  [15, 17, 18, 21, 23, 24, 26, 40, 43, 45, 46, 4...           565  \n",
       "1783  [15, 16, 19, 21, 23, 24, 43, 44, 47, 49, 51, 5...           288  \n",
       "\n",
       "[1784 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d75fadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean num features by label:\n",
      "True:\t117.55381165919283\n",
      "False:\t372.87668161434976\n"
     ]
    }
   ],
   "source": [
    "print('Mean num features by label:')\n",
    "for label in [True, False]:\n",
    "    temp = eval_dataset[eval_dataset['label']==label]\n",
    "    mean_num_features = temp['num_features'].mean()\n",
    "    print(f'{label}:\\t{mean_num_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77beea",
   "metadata": {},
   "source": [
    "## Getting num features in individual sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01fca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_in_seq(seq, scorer, dataset):\n",
    "    item = seq.strip().split(' ')\n",
    "    phonemes = [BOUNDARY] + item + [BOUNDARY]\n",
    "    encoded = dataset.vocab.encode(phonemes)\n",
    "    features = mf_scorer._featurize(encoded).nonzero()[0]\n",
    "    return len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_features_in_seq('g A I i', mf_scorer, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
