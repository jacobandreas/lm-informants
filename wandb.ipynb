{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93865d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexisjihyeross\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b833c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import wandb\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "api = wandb.Api(timeout=50)\n",
    "entity, project = \"lm-informants\", \"0704_english\"  # set to your entity and project \n",
    "runs = api.runs(entity + \"/\" + project) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332d3fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▌                                                                                                                                                          | 2/70 [00:04<02:37,  2.31s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "summary_list, config_list, name_list = [], [], []\n",
    "mega_df = pd.DataFrame()\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for run_idx, run in tqdm(enumerate(runs), total=len(runs)):\n",
    "    \n",
    "#     if run_idx < 1359:\n",
    "#         continue\n",
    "    \n",
    "    # add all the keys that are logged that you want to download\n",
    "    keys = [\n",
    "        \"step\",\n",
    "        \"auc\",\n",
    "        'strategy_used_is_train',\n",
    "        ]\n",
    "                    \n",
    "    if 'Model Eval Logs' in run.summary:\n",
    "        model_eval_logs_exists = True\n",
    "    else:\n",
    "        model_eval_logs_exists = False\n",
    "    \n",
    "    # download Model Eval Logs table\n",
    "    if model_eval_logs_exists:\n",
    "        \n",
    "        path = run.summary['Model Eval Logs']['path']\n",
    "        run.file(path).download(replace=True)\n",
    "\n",
    "        table = json.load(open(path))\n",
    "        \n",
    "        model_eval_logs = pd.DataFrame(data=table['data'], columns=table['columns'])\n",
    "#         display(df)\n",
    "#         table = run.use_artifact(f\"run-{run.id}-Model Eval Logs\").get(\"Model Eval Logs\")\n",
    "\n",
    "#         model_eval_logs = run.summary['Model Eval Logs']\n",
    "#         model_eval_logs = (run.history(keys=['Model Eval Logs'])['Model Eval Logs'][0])\n",
    "#         print(model_eval_logs)\n",
    "    \n",
    "    history = run.scan_history()\n",
    "    \n",
    "    history_df = pd.DataFrame(history)\n",
    "                \n",
    "    # filter ones that were killed\n",
    "    if run.state != \"finished\":\n",
    "        print(\"filtering run (not finished): \", run.path)\n",
    "        continue\n",
    "        \n",
    "    # get the experiment config\n",
    "    config = {k: v for k,v in run.config.items()\n",
    "         if not k.startswith('_')}\n",
    "    \n",
    "    exp = {}\n",
    "    \n",
    "    # tells us whether strategy_used_is_train_exists was logged as a metric\n",
    "    strategy_used_is_train_exists = ('strategy_used_is_train' in history_df.columns)\n",
    "    \n",
    "    # if key doesn't exist, set to nan (though this shouldn't happen after filtering empty runs)\n",
    "    exp.update({f\"{col}\": history_df[~history_df[col].isnull()][col].values if col in history_df.columns else np.nan for col in keys})\n",
    "\n",
    "    num_steps = len(exp[keys[0]])\n",
    "    for k in keys:\n",
    "        if isinstance(exp[k], float) and np.isnan(exp[k]):\n",
    "            print(f'Warning: key {k} is nan for run: ', run.path)\n",
    "            # set to list of nans\n",
    "            exp[k] = [np.nan] * num_steps\n",
    "            continue\n",
    "        # only make sure that the non-nan keys have num_steps many entries\n",
    "        assert len(exp[k]) == num_steps\n",
    "        \n",
    "    # convert dict of lists to list of dicts\n",
    "    results = [dict(zip(exp,t)) for t in zip(*exp.values())]\n",
    "    \n",
    "    for r in results:\n",
    "        r.update({f\"config/{key}\": val for key, val in config.items()})\n",
    "        r.update({f\"strategy\": run.name})\n",
    "        r.update({f\"wandb_id\": run.path})\n",
    "        \n",
    "        if model_eval_logs_exists:\n",
    "            table_row = model_eval_logs.iloc[int(r['step'])]\n",
    "            strategy_used = table_row['strategy_for_this_candidate']\n",
    "            r.update({'strategy_used': strategy_used})\n",
    "            \n",
    "            if strategy_used_is_train_exists:\n",
    "                assert r['strategy_used_is_train'] == (strategy_used == 'train'), (f\"logged strategy_used_is_train is {r['strategy_used_is_train']}\"\n",
    "                                                                              f\" but strategy_used from table is {strategy_used}\"\n",
    "                                                                              f\" for step {r['step']}\")\n",
    "            \n",
    "            # if strategy_used_is_train wasn't logged, set it with the table logged at the end\n",
    "            else:\n",
    "                r['strategy_used_is_train'] = (strategy_used == 'train')\n",
    "        \n",
    "        # strategy_used_is_train will be nan if BOTH strategy_used_is_train wasn't directly logged AND Model Eval Logs wasn't logged (it defaults to nan if not in the columns when looping through keys)\n",
    "        # strategy_used will be nan if model_eval_logs_exists = False (i.e. the Model Eval Logs table wasn't logged)\n",
    "        else:\n",
    "            r['strategy_used'] = np.nan\n",
    "        \n",
    "        \n",
    "            \n",
    "                \n",
    "    all_data.extend(results)\n",
    "    \n",
    "    \n",
    "# get status\n",
    "    \n",
    "mega_df = pd.DataFrame(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = './runs.csv'\n",
    "mega_df.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e1030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
